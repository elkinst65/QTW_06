---
title: "MSDS 7333 Case Study 06"
author: "Matthew Baldree, Ben Brock, Tom Elkins, Austin Kelly"
date: "June 17, 2017"
output:
  html_notebook: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Table of Contents:
<li>[Assignment Requirements](#AssignmentOutline)</li>
<li>[Introduction](#Introduction)</li>
<li>[**Part 1**](#Part1)</li> 
<ul> 
  <li>[Signal Collection](#SigCol)</li>
  <li>[Signal Strength](#SigStren)</li>
  <li>[Signal Density](#SigDen)</li>
  <li>[Discussion](#Discussion)</li>
  <li>[k-Nearest Neighbors](#knn)</li>
</ul>
<li>[**Part 2**](#Part 2)</li>

<li>[References](#References)</li>
<li>[Appendix](#Appendix)</li>


<a id='AssignmentOutline'>

### <b>Assignment</b> </a>
1. Conduct a more thorough data analysis into these two MAC addresses, including determining locations by using data corresponding to both the chosen and rejected Access Points:
    + *Which of these two MAC addresses should be used and which should not be used for Real Time Location System (RTLS)?*
    + *Which MAC address yields the best prediction of location? (Including determining locations by using data corresponding to both MAC addresses)*
    + *Does using data for both MAC addresses simultaneously yield more, or less accurate prediction of location?*

2. Implement alternative k-nearest precition method using weights on received signal strength.
    + *For what range of values of weights are you able to obtain better prediction values than for the unweighted approach?*
    + *Use calcError() to compare approaches*

***  

<a id='Introduction'>

### <b>Introduction</b></a>  
#### *Explain the case study*

<font color = "purple"><b>>>>>Everything in this color to be removed at final review<<<<</b></font>

<font color = "purple"><b>
- Discuss the goal of the case study
</b></font>

For this case study, the overall goal is to use K-Nearest Neighbors (kNN) and an alternative weighted method to be able to determine how different handheld devices moved through a building <font color = "red">(at Dartmouth. Let's confirm this and add some more detail)</font>. This can be achieved with an analysis of how the different Media Access Control (MAC) addresses for each of the devices interacted with the Access Points (APs) of the building. In this case, the APs are routers with static locations throughout the building. 

<font color = "purple"><b>
- Discuss the different Access points and MAC Addresses
</b></font>

The access points as stated above allow for the tracking of the devices thanks to the logged history of times, signal strengths and angles for each MAC address with respect to each access point.<font color = "red">&larr;(consider fragmenting this sentence)</font> From the information that has been given, there are known to be six APs on the building floor in question. However, this is a problem since there are seven APs recorded. To determine which AP to exclude from the analysis, we will need to compare these two addresses:

    chosenAP   = "00:0f:a3:39:e1:c0"
    rejectedAP = "00:0f:a3:39:dd:cd"

and decide which to exclude and why.

<font color = "purple"><b>
- Discuss why we're trying to remove one
</b></font>

Removing one of these APs will allow us to remove noise from the resulting model. Since only six APs are recorded as existing on the floor, the presence of a seventh points to external interference. Once the superfluous address has been identified, we will execute testing to determine whether there will be any benefit added to the model by keeping or excluding said data. This decision will ultimately lead to a better prediction of location of each device within this floorplan. 

<font color = "purple"><b>
- BRIEF overview of K-Nearest Neighbors
</b></font>

K-Nearest Neighbors (kNN) is an ideal method to use in this case as the goal is to be able to predict the path of each device on their journey through the building. To utilize this method, we use separate testing and training data. The training data are known observations from well documented locations of both the handheld devices and APs. These locations are represented by the angle at which the AP received the signal from the device as well as the overall signal strength received.

With this training data, each observation is evaluated along with the observations which occur in the general vicinity in the training data. These occurrences allow for the different probabilities of occurrence to be proposed through the model. Once this model has been generated for k neighbors, it is then tested against the unobserved test data and adapted to suit what actually occurs. This allows for an accurate gauge of accuracy of the model and is recreated for a span of possible k neighbors to exhaust the total possible number of observations. 

<font color = "purple"><b>
- Discuss the alternative weighted method. 
</b></font>

In contrast to the kNN method, another option to explore is a weighted method based on the signal strength received at the access point. This method involves defining a new metric where the weight of each observation is inversely proportional to the distance from the test observation. From a topographical standpoint, the observations which are closest to the observation will have a much higher influence than those which exist further away. 

<font color = "purple"><b>
- Discuss potential for prediction and what we expect to come of this.
</b></font>

The evaluation of this data will allow us to determine which data will be critical to the creation of an accurate model and which will not. Since we have six access points and seven addresses, this thorough investigation will help us decide which address is simply an external interference and if it is beneficial to keep involving said data. As a result, we expect to have a more thorough understanding of the difference between k-Nearest Neighbors and the weighted alternative. 


***
<a id='Part1'>

### Part 1</a>  
#### The Approach (Explained)

Before we begin, we will need to set the parameters for printing and formatting
```{r}
options(digits = 2)
options(error = recover, warn = 2)
```

Before continuing, we will need to address another file which houses the predefined functions. This helps to save notebook real estate and prevents confusion:
```{r}
source("MSDS7333-baldree-case6-fx.r", print.eval = TRUE)
```

Read offline data
```{r}
offline = readData()
```

Conduct a more thorough data analysis into these two MAC addresses, including determining locations by using data corresponding to both:
    
    * chosenAP = "00:0f:a3:39:e1:c0"
    * rejectedAP = "00:0f:a3:39:dd:cd"
    
Since we have the two MAC addresses in question, we will declare these addresses as variables to simplify later code. 

```{r}
chosenAP = "00:0f:a3:39:e1:c0"
rejectedAP = "00:0f:a3:39:dd:cd"
remainingAP = unique(offline[!offline$mac %in% c(chosenAP, rejectedAP),]$mac)
```



#### * *Which of these two MAC addresses should be used and which should not be used for Real Time Location System (RTLS)?*

To fully answer this question, we will need to explore exactly what is happening with each MAC address. The results of this exploration will define the parameters needed for such a decision.
<a id='SigCol'>

#### Here we will create a Signal Collection Map</a>
For this exercise, we will plot both the count and locations of the signals recorded for the access points. Even though it would be ideal to plot the access points(APs) directly on the map, we do not know their X and Y coordinates.

```{r}
plotSignalMaps(c(chosenAP, rejectedAP))
```

At first glance, we can see that the maps are quite similar. Unfortunately, the information gained from a visual comparison of these two diagrams does not give us a significant insight into which MAC address should be kept and which should be rejected. The next step to reach our goal will be to look at the individual Signal Strength Distributions

***
<a id='SigStren'>

#### Signal Strength Distributions</a>
Plotting the individual signal strength distributions for each AP angle will help add perspective to the information we are working with. For the sake of consistency, we will use the same stationary coordinates listed in the course book (coord : (2, 12)) <font color = "red">(Add reference here.)</font>

```{r}
df = subset(offline, posX ==2 & posY == 12 & !(mac %in% remainingAP), c(signal, angle, mac))
plotBoxplotSignalStrength(df)
```

From the literature, we know the higher the decibel-milliwatts means a stronger signal strength. A stronger signal strength indicates a closer physical position to the AP since signal strengths decrease exponentially with distance.<font color="red">Find this in the book and cite it.</font> Comparing the signal strength distributions in this chart, we see "e1:c0" has a stronger signal strength at this location than "dd:cd."

What if we compared the two addresses from the opposite side of the building? 
```{r}
# Let's focus on the fixed point (33,3)
df = subset(offline, posX == 33 & posY == 3 & !(mac %in% remainingAP), c(signal, angle, mac))
plotBoxplotSignalStrength(df)
```
Through the use of another perspective, the MAC address with the weaker signal at (2,12) still has a relatively weak signal when viewed from an AP at the other side of the building. The evidence against the address with the lower signal strength is mounting.

***
<a id='SigDen'>

#### Time to investigate Signal Density</a>
To properly investigate the difference between these two addresses, the signal strength densities for each address should be plotted for each 45 degree angle. This will help display the defining characteristics for each address. The location used for this analysis is the (24,4) from the book. 


```{r, fig.height=10}
df = subset(offline, posX == 24 & posY == 4 & !(mac %in% remainingAP), c(signal, angle, mac))
plotDensitySignalStrength(df)
```
From this plot, it can be seen that the rejectedAP has a higher signal density at lower strength while the acceptedAP has a higher density coverage at a higher strength. Judging by the distributions themselves, it seems as if many of the plots are normally distributed while some of the other plots (i.e. "dd:cd" at 180 and "e1:c0" at 315) are heavily skewed or even bimodal. Looking at each AP from top to bottom, there seems to exist an obvious pattern with respect to angle. This would suggest the median of each distribution varies by angle and attributes to a difference in perceived strength. 

*** 

#### Average Signal Strength Distributions
To examine the distribution for all locations and angles of the two APs in question, we will generate the summary statistics. The summary statistics for all possible combinations of locations and orientations for each AP through the use of a new factor. 

```{r}
offlineSummary = createOfflineSummary(offline)
```


```{r}
offline$posXY = paste(offline$posX, offline$posY, sep="-")

# create data frames for each combination
byLocAngleAP = with(offline, by(offline, list(posXY, angle, mac), function(x) x))
```

Now, we calculate the summary statistics:
```{r}
signalSummary = lapply(byLocAngleAP, function(oneLoc) {
  ans = oneLoc[1, ]
  ans$medSignal = median(oneLoc$signal)
  ans$avgSignal = mean(oneLoc$signal)
  ans$num = length(oneLoc$signal)
  ans$sdSignal = sd(oneLoc$signal)
  ans$iqrSignal = IQR(oneLoc$signal)
  ans
})
offlineSummary = do.call("rbind", signalSummary)
```

The Standard Deviation of the average signal strength per targeted AP:

```{r}
df = subset(offlineSummary, mac %in% c(chosenAP, rejectedAP), c(sdSignal, avgSignal, mac))
plotStdDevSignalStrength(df)
```

From this figure, it is apparent that there is considerable overlap between the two distributions of signal strength. This is evidence in favor of keeping both addresses in the analysis to provide greater coverage. 

***
<a id='Discussion'>

### Discussion</a>
#### We now have enough information to address these questions before an analysis of kNN is conducted: 

* **Which MAC address yields the best prediction of location? (Include determining locations by using data corresponding to both MAC addresses)**
* **Does using data for both MAC addresses simultaneously yield a more or less accurate prediction of location?**

The evidence thus far would suggest both addresses should be kept within the analysis to increase the coverage for the entire spectrum of strengths. The address chosen to be rejected not only has a lower signal strength when measured from either end of the building, but it also shows to have a higher signal density when it is further away from the receiver. The address chosen to be kept has a higher signal density closer to the receivers with the accompanying higher signal strengths. By keeping both addresses, the model should be able to generate a more accurate prediction of the layout of the building. This theory will be tested as we tour k-Nearest Neighbors. 

***

<a id='knn'>

#### Let's dive into the K-Nearest Neighbors:</a>

<font color = "red"><b>Discuss KNN here</b></font>

First and foremost, let's display the unique MAC addresses: 
```{r}
macs = unique(offlineSummary$mac)
macs
```

With the unique MAC addresses in hand, an accurate profile of the different signal strengths reveived by angle is able to be determined. This can be done with the following code: 
```{r}
online = readData("online.final.trace.txt", subMacs = macs)
online$posXY = paste(online$posX, online$posY, sep = "-")
length(unique(online$posXY))
tabonlineXYA = table(online$posXY, online$angle)
#output shows measurements were taken at all angles throughout the floor
tabonlineXYA
```
This output shows the signal strengths at each angle. This will be useful later!
<font color = "red"><b>&larr;Explain why &larr;</b></font>

Organize the data where each AP is in a column. 
```{r}
onlineSummary = castOnline(online)
#confirm rejected AP is included
dim(onlineSummary)
names(onlineSummary)
```

<font color = "purple">
(this can probably be removed)  

> for k-NN, we will include training data with angles close to point in question since angle matters.
> if we want one angle, then include angles that match the rounded orientation of new observation.
> if we want two angles, then pick two multiples of 45 degrees that flank the new observation's orientation.
> if we want three angles, then pick the closest 45 degree increment and on either side of it.
> what if we didn't want to collapse the signal strengths across the m angles, and
> instead return a set of mx166 signals for each access point?
</font>

When it comes to the kNN, we will need to include the data which include the angles close to the point in question. As observed before, the angle matters! For this exercise, we will need to abide by the following outlines: 
  
  * If one angle is desired, then we will include angles which match the rounded orientation of the new observation  
  * If we want two angles, two multiples of 45 degrees will be chosen in order to encapsulate the nw observation's orientation  
  * For three angles, the nearest 45 degree increment is chosen and either side will be used.   
  
What if we didn't want to collapse the signal strengths across the *m* angles and, instead, we return a set of mx166 signals for each access point? 

Here, we begin by iterating through 1-3 neighbors at 1-3 angles for each combination to determine which will be best suited for cross-validation: 

```{r}
for (ap in c("None", rejectedAP, chosenAP)){
  if (ap == "None"){
    test = onlineSummary
    train = offlineSummary
    newSignals = test[ , 6:12]
  } else {
    test = onlineSummary[, !(names(onlineSummary) %in% ap)]
    train = subset(offlineSummary, mac != ap)
    newSignals = test[,6:11]
  }
  
  estXY = predXY(newSignals = newSignals,
                 newAngles = test[, 4],
                 train, 
                 numAngles = 3, k = 3)
  actXY = test[ , c("posX", "posY")]
  err = calcError(estXY, actXY)
  
  print(paste(ap, err))
}
```

<font color="red"><b>&uarr;We will need to discuss these results&uarr;</b></font>


<font color = "green" size = "5px"><b>This is where we will perform cross validation and plot for desired AP for a range of k's </b></font>

Perform cross validation and plot for desired AP for a range of k's for each fold. We want to find k-NN estimates from 1 to k and aggregate errors over the v folds. 

Choose a v-fold of 11
```{r}
v = 11
```

### **Here be Dragons &darr;**

```{r}
# number of locations
permuteLocs = sample(unique(offlineSummary$posXY))

```

```{r}
# matrix locations into folds
permuteLocs = matrix(permuteLocs, ncol = v, nrow = floor(length(permuteLocs)/v))
```
<font color = "Red"><b>&uarr; come back to this one &uarr; For some reason it only runs after it throws an error (e.g.change matrix to sapply, run, change back and run again</b></font>

```{r}
# summarize and format offline
keepVars = c("posXY", "posX", "posY", "orientation", "angle")
onlineCVSummary = reshapeSS(offline, keepVars = keepVars, sampleAngle = TRUE)
```


```{r}
# number of k's
K = 10
# errors array
err = rep(0, K)

# loop through folds
for (j in 1:v) {
  testFold = subset(onlineCVSummary, posXY %in% permuteLocs[ , j])
  trainFold = subset(offlineSummary, posXY %in% permuteLocs[ , -j])
  actFold = testFold[ , c("posX", "posY")]

  # loop through neighbors
  for (k in 1:K) {
    estFold = predXY(newSignals = testFold[ , 6:12],
                     newAngles = testFold[ , 4],
                     trainFold, numAngles = 3, k = k)
    err[k] = err[k] + calcError(estFold, actFold)
  }
}
# plot k to sum of squared errors
plotSSErrors(err, K)
```



```{r}
# experiment with a few k's based on above analysis to determine k
# that results in lowest sum squared error on test data.
estXY = predXY(newSignals = onlineSummary[ , 6:12],
               newAngles = onlineSummary[ , 4],
               offlineSummary, numAngles = 3, k = 5)
actXY = onlineSummary[ , c("posX", "posY")]
calcError(estXY, actXY)
```

*** 
<a id='Part2'>

#Part 2: Alternative k-Nearest Neighbors
</a>
```{r}
# create weights for neighbors based on signal strength and multiple times X and Y and sum neighbors
# to get X and Y.
estXY = predXY(newSignals = onlineSummary[ , 6:12],
               newAngles = onlineSummary[ , 4],
               offlineSummary, numAngles = 3, k = 5, weighted=TRUE)
actXY = onlineSummary[ , c("posX", "posY")]
calcError(estXY, actXY)
```

***
<a id='References'>

### References  
</a>

<a id='Appendix'>

### Appendix (including function code)  
</a>
This is the code used to define the functions used within the study. It can be found under 'MSDS7333-baldree-case6-fx.r' which is included in the final submission  
```{r}
processLine = function(x)
{
  # Process a line of text returning a 11x10 matrix of tokens
  #
  # Args:
  #   x: The line of text
  #
  tokens = strsplit(x, "[;=,]")[[1]]

  # return NULL if only 10 tokens
  if (length(tokens) == 10)
    return(NULL)

  tmp = matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  mat = cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, byrow = TRUE), tmp)
}

roundOrientation = function(angles) {
  # Round an array of degree angles to the nearest 45 degree angle.
  #
  # Args:
  #   angles: The line of text
  #
  refs = seq(0, by = 45, length  = 9)
  q = sapply(angles, function(o) which.min(abs(o - refs)))
  c(refs[1:8], 0)[q]
}


readData = function(filename = 'offline.final.trace.txt',
                    subMacs = c("00:0f:a3:39:e1:c0", "00:0f:a3:39:dd:cd", "00:14:bf:b1:97:8a",
                                "00:14:bf:3b:c7:c6", "00:14:bf:b1:97:90", "00:14:bf:b1:97:8d",
                                "00:14:bf:b1:97:81")) {
  # Process the csv file peforming the following steps:
  #   Read all non-comment lines.
  #   Call processLine() for each line of text.
  #   Convert list of text into a data frame
  #   Name columns of data frame
  #   Define column types
  #   Convert millisecond time to seconds
  #   Round angle orientations
  #   Drop data
  #
  # Args:
  #   filename: The name of the csv file to process. The default is 'offline.final.trace.txt.'
  #   subMacs: The list of access point MAC addresses to filter the dataset down to. A default list is provided.
  #
  # Returns:
  #   The data frame
  #

  # process csv into matrix of tokens
  txt = readLines(filename)
  lines = txt[substr(txt, 1, 1) != "#"]
  tmp = lapply(lines, processLine)
  # convert into tokens into a data frame
  df = as.data.frame(do.call("rbind", tmp), stringsAsFactors = FALSE)

  # variable names
  names(df) = c("time", "scanMac", "posX", "posY", "posZ", "orientation", "mac", "signal", "channel", "type")

  # convert numeric values
  numVars = c("time", "posX", "posY", "orientation", "signal")
  df[numVars] = lapply(df[numVars], as.numeric)

  # convert time to POSIX
  df$rawTime = df$time
  df$time = df$time / 1000
  class(df$time) = c("POSIXt", "POSIXct")

  # round orientations to nearest 45
  df$angle = roundOrientation(df$orientation)

  # keep only signals from access points
  df = df[df$type == "3",]
  df = df[, "type" != names(df)]

  # drop scanMac, posZ, channel, and type - no info in them
  df = df[, !(names(df) %in% c("scanMac", "posZ"))]

  # drop more unwanted access points
  df = df[df$mac %in% subMacs,]

  # drop channel
  df = df[, "channel" != names(df)]

  return(df)
}

plotBoxplotSignalStrength = function(df) {
  # Box plot signal strength
  #
  # Args:
  #   df: data frame of data to analyze
  #
  oldPar = par(mar = c(3.1, 3.1, 1, 1), mfrow = c(1,1))
  par(mai=c(.8, .8, .5, .25))
  library (lattice)

  print(bwplot(signal ~ factor(angle) | mac, data = df, layout = c(2,1),
               main="Signal Strength Distribution for Access Points",
               xlab="Angle Measured (deg)", ylab="Signal Strengh (dBM)"))
  par(oldPar)
}

plotDensitySignalStrength = function(df) {
  # Density plot signal strength
  #
  # Args:
  #   df: data frame of data to analyze
  #
  oldPar = par(mar = c(3.1, 3.1, 1, 1), mfrow = c(1,1))
  par(mai=c(.8, .8, .5, .25))
  library (lattice)

  print(densityplot( ~ signal | mac + factor(angle), data = df, bw = 0.5, plot.points = FALSE,
        main="Signal Strength Distribution for Access Points",
        xlab="Angle Measured (deg)", ylab="Signal Strengh (dBM)"))
  par(oldPar)
}

plotStdDevSignalStrength = function(df) {
  # Standard Deviation of signal strength average
  #
  # Args:
  #   df: data frame of data to analyze
  #
  oldPar = par(mar = c(3.1, 3.1, 1, 1), mfrow = c(1,1))
  par(mai=c(.8, .8, .5, .25))
  library (lattice)
  breaks = seq(-90, -30, by=5)

  print(bwplot(sdSignal ~ cut(avgSignal, breaks=breaks) | mac, data=df,
               xlab = "Mean Signal (dBM)", ylab = "Signal Standard Deviation (dBM)",
               main = "Standard Deviation of Average Signal Strength from Detector to Access Point",
               cex.main=.8, cex.axis=.8, cex.lab=.8, layout = c(1, 2)))
  par(oldPar)
}


plotSignalMaps = function(macAddresses) {
  # Plot the number of signals per XY location at XY location.
  #
  # Args:
  #   macAddresses: a list of macs to plot signals
  #
  oldPar = par(mar = c(3.1, 3.1, 1, 1), mfrow = c(1,1))
  par(mfrow = c(2, 1), mai=c(.8, .8, .4, .25))

  for (i in macAddresses) {
    # filter data set
    df = offline[offline$mac %in% i, ]
    # location of datapoints filtered
    locDF = with(df, by(df, list(posX, posY), function(x) x))
    # drop locations that were not observed, null
    locDF = locDF[!sapply(locDF, is.null)]

    # determine the number of observations recorded at each location
    locCounts = sapply(locDF, nrow)

    # keep position information with location
    locCounts = sapply(locDF, function(df) c(df[1, c("posX", "posY")], count = nrow(df)))

    # transpose matrix
    locCounts = t(locCounts)
    plot(locCounts, type = "n", xlab = "Position X", ylab = "Position Y",
         main = paste("Total Signals from Detector to Access Point: ", i),
         cex.main=.8, cex.axis=.8, cex.lab=.8)
    text(locCounts, labels = locCounts[, 3], cex = .6, srt = 45)
  }
  par(oldPar)
}

castOnline = function(df) {
  # Cast dataframe of online data so each access point (AP) is in its own column.
  # Matrix is now a 1x7 since we are including rejected AP.
  #
  # Args:
  #   df: online data frame
  #
  keepVars = c("posXY", "posX","posY", "orientation", "angle")
  numOfAPs = length(unique(df$mac))
  byLoc = with(df,
               by(df, list(posXY),
                  function(x) {
                    ans = x[1, keepVars]
                    avgSS = tapply(x$signal, x$mac, mean)
                    y = matrix(avgSS, nrow = 1, ncol = numOfAPs,
                               dimnames = list(ans$posXY, names(avgSS)))
                    cbind(ans, y)
                  }))
  return(do.call("rbind", byLoc))
}

createOfflineSummary = function(df) {
  # In order to examine distribution for all locations, angles, and are two interested APs, we
  # will create a summary statistics for all location-orientation-AP combinations with a new factor.
  # For each combination there are around 100 observations.
  #
  # Args:
  #   df: offline data frame
  #
  df$posXY = paste(df$posX, df$posY, sep="-")

  # create data frames for each combination
  byLocAngleAP = with(df, by(df, list(posXY, angle, mac), function (x) x))

  # summary statistic
  signalSummary = lapply(byLocAngleAP, function(oneLoc) {
    ans = oneLoc[1, ]
    ans$medSignal = median(oneLoc$signal)
    ans$avgSignal = mean(oneLoc$signal)
    ans$num = length(oneLoc$signal)
    ans$sdSignal = sd(oneLoc$signal)
    ans$iqrSignal = IQR(oneLoc$signal)
    ans
  })
  return(do.call("rbind", signalSummary))
}

reshapeSS = function(df,
                     varSignal = "signal",
                     keepVars = c("posXY", "posX", "posY"),
                     sampleAngle = FALSE,
                     refs = seq(0, 315, by=45)) {
  # reshape signal strength help function
  # aggregate signal strengths from these angles and create a data structure similar to onlineSummary
  # Args:
  #   df: dataframe to reshape
  #   varSignal: variable to aggregate
  #   keepVars: variables to retain
  #   sampleAngle: true if we want to select one angle at random from df
  #   refs: angle references
  #
  numOfAPs = length(unique(df$mac))
  byLocation = with(df, by(df, list(posXY),
                             function(x) {
                               # select one angle at random for each location
                               if (sampleAngle) {
                                 x = x[x$angle == sample(refs, size = 1), ]}
                               ans = x[1, keepVars]
                               avgSS = tapply(x[, varSignal], x$mac, mean)
                               y = matrix(avgSS, nrow = 1, ncol = numOfAPs,
                                          dimnames = list(ans$posXY, names(avgSS)))
                               cbind(ans, y)
                             }))
  return(do.call("rbind", byLocation))
}

selectTraininingData = function(newObservationAngle, df = NULL, m = 1){
  # select observations from df to analyze aggregate signal strengths
  # from these angles and create a data structure similar to onlineSummary.
  #
  # Args:
  #   newObservationAngle: the angle of the new observation
  #   df: offline summary data frame
  #   m: number, between 1 and 5, of angles to keep

  # m is the number of angles to keep between 1 and 5
  refs = seq(0, by = 45, length  = 8)
  nearestAngle = roundOrientation(newObservationAngle)

  # handle odd and even m number
  if (m %% 2 == 1)
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
  else {
    m = m + 1
    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)
    if (sign(newObservationAngle - nearestAngle) > -1)
      angles = angles[ -1 ]
    else
      angles = angles[ -m ]
  }
  # map angles to values in refs
  # negative angles and angles greater than 360 are mapped to appropriate angles; e.g., -45 maps to 335 and 405 maps to 45
  angles = angles + nearestAngle
  angles[angles < 0] = angles[ angles < 0 ] + 360
  angles[angles > 360] = angles[ angles > 360 ] - 360
  angles = sort(angles)
  # select observations to analyze
  subset = df[df$angle %in% angles, ]
  # reshape signal strength
  return(reshapeSS(subset, varSignal = "avgSignal"))
}

findNN = function(newSignal, trainingSubset) {
  # want to look the distance in terms of signal strengths from these training data to the new
  # data point. we need to calculate teh distrance from the new point to all observations in the
  # training set with findNN().
  # returns locations of the training observations in order of closeness to the new observation's signal strength.
  #
  # Args:
  #   newSignal: signal of new observation
  #   trainingSubset: training data to find neighbors
  # Returns:
  #   training neighbors
  #
  cols = length(trainingSubset)
  diffs = apply(trainingSubset[ , 4:cols], 1, function(x) x - newSignal)
  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )
  #closest = order(dists)
  closest = cbind(trainingSubset[, 1:3 ], dists)
  return(closest[order(dists),])
  #return(trainingSubset[closest, 1:3 ])
}

predXY = function(newSignals, newAngles, training, numAngles = 1, k = 3, weighted=FALSE){
  # Predict the XY coordinates given a list of signals along with their angles measured and training data.
  # k neighbors will be found to make the prediction of coordinate.
  #
  # Args:
  #   newSignals: list of signals to predict
  #   newAngles: list of angles for signals to predict
  #   trainingData: data needed for k-nn
  #   numAngles: angles we want to use for prediction
  #   k: number of neighbors to use for prediction
  #   weighted: if TRUE, distances to neighbors will be used to weight XY for each neighbor to determine
  #             location.
  #
  closeXY = list(length = nrow(newSignals))

  for (i in 1:nrow(newSignals)) {
    trainSS = selectTraininingData(newAngles[i], training, m = numAngles)
    closeXY[[i]] = findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)
  }
  # for some k of nearest neighbors, simply average the first k locations
  # could have used weights in the average that are inversely proportional to the distance in signal strength
  # from the test observation. will need to call findNN() to get distance to point. the weights
  # are 1/d / sum(1/d).
  # could also use a different metric besides Euclidean like Manhattan.
  # could use medians instead of averages when combining neighbors if the distribution of values are quite skewed.
  if (!weighted) {
    estXY = lapply(closeXY, function(x) sapply(x[ , 2:3], function(x) mean(x[1:k])))
  } else {
    # sum X and Y weighted neighbor values.
    estXY = lapply(closeXY, function(x) {
      wt = (1/x[1:k, 4])/sum(1/x[1:k, 4])
      xy = cbind(x[1:k,], wt)
      # sum weighted neighbors
      return(colSums(xy[1:4, 2:3] * xy[1:4, 4]))
    })
  }
  return(do.call("rbind", estXY))
}

calcError = function(estXY, actualXY) {
  # compare fit numerically with sum of squared errors
  #
  # Args:
  #   estXY:
  #   actualXY:
  # Returns:
  #   calculated residual sum of squares
  return(sum(rowSums((estXY - actualXY) ^ 2)))
}

plotSSErrors = function(err, K){
  # Plot results as sum of squared errors as function of k
  #
  # Args:
  #   err: Error array
  #   K: number of neighbors
  #

  oldPar = par(mar = c(3.1, 3.1, 1, 1), mfrow = c(1,1))
  par(mai=c(.8, .8, .4, .25))

  plot(y = err, x = (1:K),  type = "l", lwd= 2, ylim = c(900, 2100), cex.main=.8, cex.axis=.8, cex.lab=.8,
       xlab = "Number of Neighbors",
       ylab = "Sum of Squared Errors",
       main = "Cross Validation Section of k")

  rmseMin = min(err)
  kMin = which(err == rmseMin)[1]
  segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), lty = 2, lwd = 2)
  segments(x0 = kMin, x1 = kMin, y0 = 800,  y1 = rmseMin, col = grey(0.4), lty = 2, lwd = 2)
  text(x = kMin - 2, y = rmseMin + 40, label = as.character(round(rmseMin)), col = grey(0.4), cex = .6)

  par(oldPar)
}

wt_signals <- function(ByAvgSig){
  
  # Name: wt_signals
  # Create weighted data
  #
  # Args:
  #   ByAvgSig:    estimate XY line segment
  #   
  # Returns:
  #   log_ang by weighted_signals matrix
  
  # calculate signals
  signals <- select(ByAvgSig, -posXY, -posX, -posY, -angle) 
  
  # calculate log_ang
  loc_ang <- select(ByAvgSig, posXY, posX, posY, angle)
  
  # calculate weighted_signals 
  weighted_signals <- (1/signals) / rowSums(1/signals)
  
  return(cbind(loc_ang, weighted_signals))
}
```